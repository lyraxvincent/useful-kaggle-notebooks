{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Tuning Hyperparameters with Optuna",
   "metadata": {
    "tags": [],
    "is_collapsed": false,
    "cell_id": "00000-eeae5232-4af0-45a3-974e-80dcaca7d9d5",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 6,
     "w": 24,
     "h": 2
    },
    "deepnote_cell_type": "text-cell-h1"
   }
  },
  {
   "cell_type": "markdown",
   "source": "This is an example on how to setup Optuna to tune hyperparameters for two different models. This example is part of [My First Kaggle Competition](https://deepnote.com/publish/c734d949-9d72-45ad-9b32-d13706be706f).\n\n<img src='https://images.unsplash.com/photo-1554696468-19f8c7a71ad5' alt='Hyperparameter Tuning'>",
   "metadata": {
    "tags": [],
    "cell_id": "00001-85104a93-741f-4e0e-9220-aa5581c7ce7e",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 9,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nimport optuna\n\nfrom sklearn import compose\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn import pipeline\nfrom sklearn import preprocessing\n\nimport xgboost as xgb\nimport catboost as cat\n\n\n# This is nice handy constant to turn on and off the GPU. When `False`\n# the notebook will ignore the GPU even when present.\nGPU_ENABLED = True",
   "metadata": {
    "tags": [],
    "cell_id": "00002-492f1a58-2f80-4d4e-a8e8-475537f53d5f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "31eeb97c",
    "execution_start": 1630518922651,
    "execution_millis": 9258,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 15,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "There's a lot of data, so I'm going to load 5% only to illustrate how the notebook works without having to wait an eternity for it.",
   "metadata": {
    "tags": [],
    "cell_id": "00003-e37ab895-909d-433b-9a33-432d43193786",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 45,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "train = pd.read_csv(\"train.csv\").sample(frac=0.10, random_state=42)\n\ncont_features = [f for f in train.columns.tolist() if f.startswith('cont')]\ncat_features = [f for f in train.columns.tolist() if f.startswith('cat')]\n\ny = train.target\nX = train",
   "metadata": {
    "tags": [],
    "cell_id": "00003-fe1de78f-d0d7-4da9-9251-b2bc67ebfe0d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "7e2c0f53",
    "execution_start": 1630518931920,
    "execution_millis": 1435,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 33,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": "Let's define the preprocessing transformations that I will use with the original data.\n\nNotice that there are only two different transformations that I will be using:\n\n1. Scaling values using a Min-Max Scaler.\n2. Transforming categorical columns to ordinal values.\n",
   "metadata": {
    "tags": [],
    "cell_id": "00005-02d26f83-f956-4bdc-bdd0-f417602584be",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 51,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "numerical_preprocessor = pipeline.Pipeline(steps=[\n    (\"imputer\", impute.SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", preprocessing.MinMaxScaler())\n])\n\ncategorical_preprocessor = pipeline.Pipeline(steps=[\n    (\"imputer\", impute.SimpleImputer(strategy=\"most_frequent\")),\n    (\"ordinal\", preprocessing.OrdinalEncoder())\n])\n\npreprocessor = compose.ColumnTransformer(\n    transformers=[\n        (\"numerical_preprocessor\", numerical_preprocessor, cont_features),\n        (\"categorical_preprocessor\", categorical_preprocessor, cat_features)\n    ]\n)",
   "metadata": {
    "tags": [],
    "cell_id": "00003-8eb142b0-dce6-413e-a6a2-c1ec96428609",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "19a90945",
    "execution_start": 1630518934427,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 21,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": "This is an utility function to avoid duplicating the code when exploring different models. I'll use this function from each one of the objective functions defined later in this notebook.",
   "metadata": {
    "tags": [],
    "cell_id": "00007-fbf52a6c-6c1d-4349-95e0-01a689b62750",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 57,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-5857a15f-71bf-4ed9-aebb-6513ac451ffe",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "2b7da130",
    "execution_start": 1630518935965,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": null,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "source": "def train_model_for_study(X, y, model):\n    X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n        X, \n        y, \n        test_size=0.20, \n        random_state=42\n    )\n\n    X_train = preprocessor.fit_transform(X_train, y_train)\n    X_valid = preprocessor.transform(X_valid)\n\n    model.fit(\n        X_train, \n        y_train,\n        early_stopping_rounds=300,\n        eval_set=[(X_valid, y_valid)], \n        verbose=False\n    )\n\n    yhat = model.predict(X_valid)\n    return metrics.mean_squared_error(y_valid, yhat, squared=False)",
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "This is the objective function to tune an `XGBRegressor` model. Notice how this function uses the `train_model_for_study()` function that we defined before.",
   "metadata": {
    "tags": [],
    "cell_id": "00009-6cdfc1b8-0705-401c-85ba-9e6880670931",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 63,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def objective_xgb(trial):\n    \"\"\"\n    Objective function to tune an `XGBRegressor` model.\n    \"\"\"\n\n    params = {\n        'n_estimators': trial.suggest_int(\"n_estimators\", 1000, 10000),\n        'reg_alpha': trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1.0, log=True),\n        'max_depth': trial.suggest_int(\"max_depth\", 2, 9),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n    }\n\n    if GPU_ENABLED:\n        params[\"tree_method\"] = \"gpu_hist\"\n        params[\"predictor\"] = \"gpu_predictor\"\n\n    model = xgb.XGBRegressor(\n        booster=\"gbtree\",\n        objective=\"reg:squarederror\",\n        random_state=42,\n        **params\n    )\n\n    return train_model_for_study(X, y, model)",
   "metadata": {
    "tags": [],
    "cell_id": "00010-58904aef-0fbc-4c7b-a107-d8b27b02efae",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "634aeaba",
    "execution_start": 1630518937801,
    "execution_millis": 4,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 69,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "And this is the objective function to tune a `CatBoostRegressor` model. Notice how this function also uses the `train_model_for_study()` function that we defined before.",
   "metadata": {
    "tags": [],
    "cell_id": "00011-fea8ab47-703c-4b34-86bb-887d41910542",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 75,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "def objective_cat(trial):\n    \"\"\"\n    Objective function to tune a `CatBoostRegressor` model.\n    \"\"\"\n\n    params = {\n        'iterations':trial.suggest_int(\"iterations\", 4000, 25000),\n        'od_wait':trial.suggest_int('od_wait', 500, 2300),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.01, 1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n        'subsample': trial.suggest_uniform('subsample',0,1),\n        'random_strength': trial.suggest_uniform('random_strength',10,50),\n        'depth': trial.suggest_int('depth',1, 15),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n    }\n\n    if GPU_ENABLED:\n        params[\"task_type\"] = \"GPU\"\n        params[\"bootstrap_type\"] = \"Poisson\"\n\n    model = cat.CatBoostRegressor(\n        loss_function=\"RMSE\",\n        random_state=42,\n        **params,\n    )\n    \n    return train_model_for_study(X, y, model)",
   "metadata": {
    "tags": [],
    "cell_id": "00012-59d28c19-2987-43af-a92f-5a0335a46de5",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "d69c5da6",
    "execution_start": 1630518940376,
    "execution_millis": 1,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 81,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": "We can now run the study for the `XGBRegressor` model and display the best set of hyperparameters when it finishes. Here, I'm running 5 trials only, which is not enough to find a good set of hyperparameters. During the competition, I ended up running 300 trials.",
   "metadata": {
    "tags": [],
    "cell_id": "00013-ebc3c264-b82e-4988-b467-f162da0d6747",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 87,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "study_xgb = optuna.create_study(direction=\"minimize\")\nstudy_xgb.optimize(objective_xgb, n_trials=5)\nstudy_xgb.best_params",
   "metadata": {
    "tags": [],
    "cell_id": "00006-514874f9-8020-4abe-9cd3-95a4a4b34b3f",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "82526fe8",
    "execution_start": 1630518942728,
    "execution_millis": 68690,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 27,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001b[32m[I 2021-09-01 17:55:42,724]\u001b[0m A new study created in memory with name: no-name-0e3cc451-aa00-406c-93f5-ef1f11526c9c\u001b[0m\n\u001b[32m[I 2021-09-01 17:55:48,279]\u001b[0m Trial 0 finished with value: 0.738416091458543 and parameters: {'n_estimators': 7992, 'reg_alpha': 1.3133580413265216e-06, 'reg_lambda': 16.290093746736527, 'subsample': 0.6, 'learning_rate': 0.16027908156522835, 'max_depth': 5, 'colsample_bytree': 0.6595439362298875}. Best is trial 0 with value: 0.738416091458543.\u001b[0m\n\u001b[32m[I 2021-09-01 17:55:58,392]\u001b[0m Trial 1 finished with value: 0.7373312498627275 and parameters: {'n_estimators': 5838, 'reg_alpha': 4.396167769884513e-08, 'reg_lambda': 5.326525015477008e-05, 'subsample': 0.7, 'learning_rate': 0.06114842392791542, 'max_depth': 7, 'colsample_bytree': 0.42392061015953364}. Best is trial 1 with value: 0.7373312498627275.\u001b[0m\n\u001b[32m[I 2021-09-01 17:56:17,568]\u001b[0m Trial 2 finished with value: 0.7902618092751004 and parameters: {'n_estimators': 6637, 'reg_alpha': 1.2146118678935444e-07, 'reg_lambda': 1.0920854782930351e-05, 'subsample': 0.8, 'learning_rate': 0.5532814143906422, 'max_depth': 9, 'colsample_bytree': 0.7532729084007421}. Best is trial 1 with value: 0.7373312498627275.\u001b[0m\n\u001b[32m[I 2021-09-01 17:56:25,627]\u001b[0m Trial 3 finished with value: 0.7358492670026711 and parameters: {'n_estimators': 4955, 'reg_alpha': 0.0031430412204915526, 'reg_lambda': 1.1775420703839643, 'subsample': 0.7, 'learning_rate': 0.09235910120865394, 'max_depth': 6, 'colsample_bytree': 0.4168448487051154}. Best is trial 3 with value: 0.7358492670026711.\u001b[0m\n\u001b[32m[I 2021-09-01 17:56:51,362]\u001b[0m Trial 4 finished with value: 0.7375163511817688 and parameters: {'n_estimators': 6086, 'reg_alpha': 0.48387553975401604, 'reg_lambda': 0.0072717179065714625, 'subsample': 0.6, 'learning_rate': 0.03840690780111572, 'max_depth': 9, 'colsample_bytree': 0.5856589040681989}. Best is trial 3 with value: 0.7358492670026711.\u001b[0m\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 7,
     "data": {
      "text/plain": "{'n_estimators': 4955,\n 'reg_alpha': 0.0031430412204915526,\n 'reg_lambda': 1.1775420703839643,\n 'subsample': 0.7,\n 'learning_rate': 0.09235910120865394,\n 'max_depth': 6,\n 'colsample_bytree': 0.4168448487051154}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": "This runs the study for the `CatBoostRegressor` model and displays the best set of hyperparameters when it finishes. Here, I'm running 1 trial only, which is not enough to find a good set of hyperparameters. During the competition, I ended up running 300 trials.",
   "metadata": {
    "tags": [],
    "cell_id": "00015-9f87f041-de16-4557-9ff2-b9ed646cc68f",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 93,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": "study_cat = optuna.create_study(direction=\"minimize\")\nstudy_cat.optimize(objective_cat, n_trials=1)\nstudy_cat.best_params",
   "metadata": {
    "tags": [],
    "cell_id": "00008-0dd9a073-22af-4548-9269-406de19e452d",
    "deepnote_to_be_reexecuted": false,
    "source_hash": "b8c2be3",
    "execution_start": 1630520014989,
    "execution_millis": 20213,
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 39,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001b[32m[I 2021-09-01 18:13:34,984]\u001b[0m A new study created in memory with name: no-name-da3b86a4-9a22-45ed-af28-c382ae8df078\u001b[0m\nCustom logger is already specified. Specify more than one logger at same time is not thread safe.\u001b[32m[I 2021-09-01 18:13:55,191]\u001b[0m Trial 0 finished with value: 0.7350755084577055 and parameters: {'iterations': 24686, 'od_wait': 1507, 'learning_rate': 0.2910523692193494, 'reg_lambda': 78.53417683107563, 'subsample': 0.803401786024069, 'random_strength': 35.357782114945834, 'depth': 4, 'min_data_in_leaf': 22, 'leaf_estimation_iterations': 11}. Best is trial 0 with value: 0.7350755084577055.\u001b[0m\n",
     "output_type": "stream"
    },
    {
     "output_type": "execute_result",
     "execution_count": 9,
     "data": {
      "text/plain": "{'iterations': 24686,\n 'od_wait': 1507,\n 'learning_rate': 0.2910523692193494,\n 'reg_lambda': 78.53417683107563,\n 'subsample': 0.803401786024069,\n 'random_strength': 35.357782114945834,\n 'depth': 4,\n 'min_data_in_leaf': 22,\n 'leaf_estimation_iterations': 11}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "tags": [],
    "cell_id": "00017-498b81d1-9829-44da-beb2-6f1866645415",
    "deepnote_app_coordinates": {
     "x": 0,
     "y": 99,
     "w": 24,
     "h": 5
    },
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ea1a123d-8d2f-4e20-8f22-95f07470d557' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
   "metadata": {
    "tags": [],
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_notebook_id": "07334140-3c7f-41e3-9556-97edd1f033cb",
  "deepnote_app_layout": "article",
  "deepnote_execution_queue": []
 }
}